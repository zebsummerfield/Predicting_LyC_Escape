import h5py
import matplotlib.pyplot as plt
import matplotlib
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
import json
from functions_old import *

folder = "optimising_model/"
file = 'cat.hdf5'

# loads both the catalogue of galaxies and their variables
with h5py.File(file, 'r') as hdf:

    f_esc = np.array(hdf['f_esc_vir_full']).astype('float32')
    n_esc = np.array(hdf['Ndot_LyC_vir_full'])

    redshift = np.array(hdf['redshift_full'])
    star_mass = np.array(hdf['stellar_mass_full'])
    ssfr10 = ssfr_func(hdf['sfr_full_10'], star_mass)
    ssfr50 = ssfr_func(hdf['sfr_full_50'], star_mass)
    ssfr100 = ssfr_func(hdf['sfr_full_100'], star_mass)
    vir_mass = np.array(hdf['M_vir_full'])
    gas_mass = np.array(hdf['gas_mass_full'])
    ha_lum = np.array(hdf['ha_lum_int_full'])
    uv_lum = np.array(hdf['uv_lum_int_full'])
    gas_met = np.array(hdf['gas_met_full'])
    # star_met = np.array(hdf['star_met_full'])
    star_size = np.array(hdf['stellar_size_full'])
    sfr_size = np.array(hdf['sfr_size_full'])
    ha_size = np.array(hdf['ha_size_int_full'])
    uv_size = np.array(hdf['uv_size_int_full'])
    # uv_projection = np.array(hdf['uv_size_int_2d_full'])
    # ha_projection = np.array(hdf['ha_size_int_2d_full'])
    # uv_obs = np.array(hdf['uv_lum_obs_full'])
    sfr10 = np.array(hdf['sfr_full_10'])
    sfr10_density = sfr10 / (np.pi * sfr_size**2)

    random_variable = np.random.uniform(low=0, high=1, size=(len(f_esc)))

    f_esc_vars = np.array([ssfr10, ssfr100, star_mass, gas_mass, vir_mass, gas_met,
                           uv_lum, ha_lum, uv_size, ha_size,
                           sfr_size, star_size, sfr10_density, (1+redshift), random_variable,])
    f_esc_keys = np.array(['offset10', 'ssfr10/ssfr100', 'star_mass', 'gas_mass/star_mass', 'star_mass/vir_mass',
                           'gas_met', 'uv_mag', 'uv_lum/ha_lum', 'uv_size', 'ha_size',
                           'sfr_size', 'sfr_size/star_size', 'sfr10_density', '1 + redshift', 'random_variable'])
    
    # adds a small epsilon to the variables to avoid log(0) errors
    eps_frac = 0.1
    f_epsilons = np.array([min([v for v in var if v !=0])*eps_frac for var in f_esc_vars])
    eps_array = np.zeros(f_esc_vars.shape)
    for i in range(len(eps_array)):
        eps_array[i].fill(f_epsilons[i])
    f_esc_vars = f_esc_vars + eps_array

    # post adding epsilon, the variables are changed to match the desired form given by their key
    f_esc_vars[1] = f_esc_vars[0] / f_esc_vars[1]
    f_esc_vars[3] = f_esc_vars[3] / f_esc_vars[2]
    f_esc_vars[4] = f_esc_vars[2] / f_esc_vars[4]
    f_esc_vars[7] = f_esc_vars[5] / f_esc_vars[6]
    f_esc_vars[11] = f_esc_vars[10] / f_esc_vars[11]

    log_f_esc_vars = np.log10(f_esc_vars).astype('float32')

    # replaces ssfr10 with offset from the star forming main sequence over 10Myrs
    log_osfms10 =  log_f_esc_vars[0] - sfms_func(np.array([redshift, np.log10(star_mass)]), s[0], b[0], u[0])
    log_f_esc_vars[0] = log_osfms10.astype('float32')
    # replaces the luminosities with magnitudes
    lum_to_tenpc = 4 * np.pi * (10 * 3.086e18)**2
    uv_mag = -2.5 * np.log10((f_esc_vars[6]) / lum_to_tenpc) - 48.6
    log_f_esc_vars[5] = uv_mag.astype('float32')

    vars = f_esc_vars
    log_vars = log_f_esc_vars
    keys = f_esc_keys
    print(keys)
    
    # removes any rows that have zero ,unity or infinity for the vars and f_esc
    # ssfr50 is included to remove galaxies that have had no recent star formation
    f_esc[np.isnan(f_esc)] = 0
    for i in range(len(np.concatenate((log_vars, [f_esc], [ssfr50])))):
        nan_indices = [index for index, val in enumerate(list(np.concatenate((log_vars, [f_esc], [ssfr50]))[i]))
                       if (val == 0 or val == 1 or val == np.inf or val== -np.inf)][::-1]
        print(f"rows deleted: {len(nan_indices)}")
        f_esc, n_esc = (np.delete(f_esc, nan_indices), np.delete(n_esc, nan_indices))
        log_vars = np.delete(log_vars, nan_indices, axis=1)
        ssfr10, ssfr50, ssfr100 = (np.delete(ssfr10, nan_indices), 
                                   np.delete(ssfr50, nan_indices),
                                   np.delete(ssfr100, nan_indices))
    print(f'rows remaining: {len(f_esc)}')

    log_f_esc = np.log10(f_esc).astype('float32')
    log_n_esc = np.log10(n_esc).astype('float32')

    # each row contains the data for a single galaxy
    X = np.transpose(log_vars)
    Y = log_f_esc


# optimising the minimum number of samples allowed on the end nodes
min_leafs = np.arange(5, 301, 5)
mean_test_mae = np.zeros(len(min_leafs))
mean_test_mse = np.zeros(len(min_leafs))
mean_train_mae = np.zeros(len(min_leafs))
mean_train_mse = np.zeros(len(min_leafs))
for i_leafs in range(len(min_leafs)):

    print(f"min sample leafs: {min_leafs[i_leafs]}")
    # run random forest 10 times to get an average on importances and errors
    n = 10
    test_mae_list = np.zeros(n)
    test_mse_list = np.zeros(n)
    train_mae_list = np.zeros(n)
    train_mse_list = np.zeros(n)
    importances_list = np.zeros(shape=(n, len(keys)))

    for i_run in range(n):
        print(f"Run {i_run+1}")
        # random forest training and testing with both the test and train data
        x_train, x_test, y_train, y_test = train_test_split(
            X, Y, test_size=0.25, random_state=i_run)
        rf = RandomForestRegressor(n_estimators=100, random_state=i, n_jobs=-1,
                               min_samples_leaf=min_leafs[i_leafs], max_features='sqrt', 
                               criterion='friedman_mse')
        rf.fit(x_train, y_train)
        y_test_pred = rf.predict(x_test)
        y_train_pred =  rf.predict(x_train)

        # calculate errors on the test and train data
        test_mae_list[i_run] = mean_absolute_error(y_test, y_test_pred)
        test_mse_list[i_run] = mean_squared_error(y_test, y_test_pred)
        train_mae_list[i_run] = mean_absolute_error(y_train, y_train_pred)
        train_mse_list[i_run] = mean_squared_error(y_train, y_train_pred)

    mean_test_mae[i_leafs] = np.mean(test_mae_list)
    mean_test_mse[i_leafs] = np.mean(test_mse_list)
    mean_train_mae[i_leafs] = np.mean(train_mae_list)
    mean_train_mse[i_leafs] = np.mean(train_mse_list)

# saves the error data to a json file
with open(folder+'min_samples_leaf_optimise_data.json', 'w') as f:
    json.dump({'min_leafs': min_leafs.tolist(),
               'mean_test_mae': mean_test_mae.tolist(), 
               'mean_test_mse': mean_test_mse.tolist(), 
               'mean_train_mae': mean_train_mae.tolist(), 
               'mean_train_mse': mean_train_mse.tolist()}, f)

threshold = 0.05
p_diff = (mean_test_mae - mean_train_mae) / mean_test_mae
indices_thresh=np.where(p_diff<threshold)
print(f"minimum samples per leaf 5% threshold: {min(min_leafs[indices_thresh])}")

matplotlib.rcParams.update({'font.size': 16})
fig, ax = plt.subplots(figsize=(8,8))

ax.plot(min_leafs, mean_test_mae, label='Test MAE')
ax.plot(min_leafs, mean_train_mae, label='Train MAE')
ax.axvline(min_leafs[indices_thresh[0][0]], linestyle='dashdot', label=f'MAE {threshold}% threshold', c='r')
ax.set_xlabel('Minimum Number of Samples Allowed on End Nodes')
ax.set_ylabel('Mean Absolute Error (MAE)')
ax.set_ylim(0.2, 0.5)
ax.set_xlim(0, 300)

ax.legend()
fig.tight_layout()
plt.show()